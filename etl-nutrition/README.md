# ETL Nutrition & Alimentation

## Commandes Majeures (Windows/PowerShell)
- Se placer dans le sousâ€‘projet
  - `cd C:\Users\sore-larregain\Desktop\Workspace\Ynov\ETL\ETL\etl-nutrition`

- Docker Postgres
  - `docker compose up -d`
  - `docker compose ps`
  - `docker compose logs -f db`
  - `docker compose down`

- DÃ©pendances Python
  - `pip install -r requirements.txt`

- Variables dâ€™environnement (exemples)
  - ` $env:DB_URL = "postgresql+psycopg://user:pass@localhost:5432/food" `
  - ` $env:OFF_PAGE_SIZE = "1000" `
  - ` $env:OFF_MAX_PAGES = "50" `
  - ` $env:SMOKE_FAIL_ON_DQ = "1" `

- Test rapide (logs dÃ©taillÃ©s + rapport DQ JSON)
  - `python -m tests.run_pipeline_smoke`
  - Capturer logs: `python -m tests.run_pipeline_smoke *>&1 | Tee-Object -FilePath logs\smoke_$(Get-Date -Format yyyyMMdd_HHmmss).log`
  - Rapport: `logs\last_run_report_YYYYMMDD_HHMMSS.json`

- Flow Prefect (endâ€‘toâ€‘end)
  - `python flows/etl_daily.py`

- Ã‰tapes individuelles
  - Init schÃ©ma: `python -m database.init_db`
  - Extraction OFF: `python -m scripts.extract.openfoodfacts`
  - Consolidation: `python -m scripts.transform.consolidate_off`
  - Harmonisation: `python -m scripts.transform.harmonize_units`
  - Chargement DB: `python -m scripts.load.load_fact_tables`

- Lookup produit (scan codeâ€‘barres)
  - `python -m scripts.query.product_lookup <barcode>`

- Emplacements de donnÃ©es
  - Raw: `data\raw\YYYYMMDD\openfoodfacts\off_p*.json`
  - IntermÃ©diaire: `data\processed\tmp_products.jsonl`
  - HarmonisÃ©: `data\processed\products_harmonized.jsonl`

Ce dÃ©pÃ´t contient le **pipeline ETL** (Extractâ€“Transformâ€“Load) du projet fil rouge *Application Data Nutrition & Alimentation* (Masters Data & IA â€“ Ynov 2025â€“2026).  
ObjectifÂ : produire un **jeu de donnÃ©es propre, structurÃ© et centralisÃ©** Ã  partir de sources variÃ©es (Open Food Facts, Agribalyse, sites de recettes) pour la visualisation et lâ€™IA.

---

## ğŸš€ Pile technique
- **Python 3.11** (pandas, requests, pydantic, SQLAlchemy)
- **Orchestration**Â : Prefect
- **Base**Â : PostgreSQL 15 (ou SQLite pour dÃ©veloppement rapide)
- **QualitÃ© de donnÃ©es**Â : Great Expectations (ou Pandera)
- **Conteneurisation**Â : Docker + docker-compose
- **Logs**Â : logging Python + mÃ©tadonnÃ©es dâ€™exÃ©cution

---

## ğŸ—‚ï¸ Arborescence
```
etl-nutrition/
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ docker-compose.yml
â”œâ”€ .env.example
â”œâ”€ configs/
â”‚  â”œâ”€ sources.yaml
â”‚  â””â”€ mappings.yaml
â”œâ”€ data/
â”‚  â”œâ”€ raw/
â”‚  â””â”€ processed/
â”œâ”€ database/
â”‚  â”œâ”€ schema.sql
â”‚  â”œâ”€ seeds/
â”‚  â””â”€ loaders/
â”œâ”€ scripts/
â”‚  â”œâ”€ extract/
â”‚  â”‚  â””â”€ openfoodfacts.py
â”‚  â”œâ”€ transform/
â”‚  â”‚  â””â”€ harmonize_units.py
â”‚  â””â”€ load/
â”‚     â””â”€ load_fact_tables.py
â”œâ”€ flows/
â”‚  â””â”€ etl_daily.py
â””â”€ tests/
   â””â”€ expectations/
```
> Placez **exactement** chaque fichier comme indiquÃ© ci-dessus.

---

## ğŸ“¥ Ã‰tape 1 â€” Extraction
- Sources ciblesÂ : **Open Food Facts (OFF)**, **Agribalyse**, sites de recettes (Marmiton, 750g).
- Bonnes pratiquesÂ : respect de *robots.txt*, *rate limiting*, versionnement des schÃ©mas, stockage brut dans `data/raw/YYYYMMDD/` avec **manifest.json**.

Script fourniÂ : `scripts/extract/openfoodfacts.py`  
- ParamÃ©trable (taille de page, champs).  
- Sauvegarde des pages JSON dans `data/raw/<date>/openfoodfacts/` + `manifest.json`.

---

## ğŸ§¹ Ã‰tape 2 â€” Transformation
ObjectifsÂ : nettoyage, standardisation des unitÃ©s (g, kcal, kJ, mg), normalisation des colonnes (snake_case), enrichissement (scores).  
Script fourniÂ : `scripts/transform/harmonize_units.py`  
- Utilise `configs/mappings.yaml` pour convertir les unitÃ©s et fixer les cibles (`nutrient_targets`).  
- Enregistre un fichier harmonisÃ© dans `data/processed/products_harmonized.jsonl`.

---

## ğŸ—„ï¸ Ã‰tape 3 â€” Chargement
- SchÃ©ma **Ã©toile** minimalÂ : dimensions (produit, nutriment, source) + faits (valeurs nutritionnelles, impacts).  
- DDLÂ : `database/schema.sql`  
- LoaderÂ : `scripts/load/load_fact_tables.py` (SQLAlchemy) qui **insÃ¨re** les dimensions et pivote les nutriments vers la table de faits.

---

## ğŸ§­ Orchestration
`flows/etl_daily.py` orchestre **extract â†’ transform â†’ load** via Prefect (avec *retries*).

ExÃ©cuter localementÂ :
```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
export DB_URL="postgresql+psycopg://user:pass@localhost:5432/food"
python flows/etl_daily.py
```

Avec DockerÂ :
```bash
cp .env.example .env   # Ã©ditez les variables
docker-compose up -d   # lance Postgres + Prefect UI (optionnel)
```

---

## ğŸ” QualitÃ© & tests
- RÃ¨gles dâ€™exempleÂ : unicitÃ© de `code`, valeurs plausibles (kcal, sodium), couverture de nutriments clÃ©s.
- Ajoutez vos suites **Great Expectations** dans `tests/expectations/` et exÃ©cutez-les sur `data/processed/` et aprÃ¨s *load* (requÃªtes SQL).

---

## ğŸ” SÃ©curitÃ© & conformitÃ©
- Respect des licences et conditions dâ€™utilisation des sources.
- Secrets dans `.env` (jamais commit).
- TraÃ§abilitÃ©Â : chaque run gÃ©nÃ¨re un **manifest** (date, URL, hash, volumes).

---

## ğŸ—“ï¸ Planning (rappel)
- S1Â : setup repo, extraction OFF.
- S2Â : Agribalyse/mapping, transformations v1, DDL.
- S3Â : data quality, orchestration Prefect.
- S4Â : gel des donnÃ©es, doc & rÃ©pÃ©tition.

---

## ğŸ‘£ Commandes utiles
```bash
# Install
pip install -r requirements.txt

# Lancer un run ETL
python flows/etl_daily.py

# Inspecter la DB (exemple psql)
psql postgresql://user:pass@localhost:5432/food -c "\dt"
```
